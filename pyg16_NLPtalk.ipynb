{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything You Always Wanted to Know About NLP but Were Afraid to Ask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the accompanying slideshow [here](https://docs.google.com/presentation/d/1rYZEd7-8sZGBzg75OOPvSkIfd1FHq_d4elptiZXzJj8/edit?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contact Info\n",
    "\n",
    "Steven:          \n",
    "* Github: srbutler\n",
    "* email: [srbutler@gmail.com](mailto:srbutler@gmail.com)\n",
    "\n",
    "Max:         \n",
    "* Github: maxwell-schwartz \n",
    "* Twitter: @DeathAndMaxes\n",
    "* email: [maxwell.schwartz11@gmail.com](mailto:maxwell.schwartz11@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphology: words and what they're made of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These groups are taken from NLTK's implementation of the Snowball stemmer, which is (obviously) significantly more complex than this, and the test words are taken from NLTK's documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# always be prepared from the get-go\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgroup1 = (\"'s'\", \"'s\", \"'\")\n",
    "sgroup2 = (\"sses\", \"ied\", \"ies\", \"us\", \"ss\", \"s\")\n",
    "sgroup3 = (\"eedly\", \"ingly\", \"edly\", \"eed\", \"ing\", \"ed\")\n",
    "sgroup4 = ('ization', 'ational', 'fulness', 'ousness', 'iveness', 'tional', 'biliti', 'lessli', 'entli', 'ation', 'alism', 'aliti', 'ousli',  'iviti', 'fulli', 'enci', 'anci', 'abli', 'izer', 'ator', 'alli', 'bli', 'ogi', 'li')\n",
    "sgroup5 = ('ational', 'tional', 'alize', 'icate', 'iciti', 'ative', 'ical', 'ness', 'ize', 'ful')\n",
    "sgroup6 = ('ement', 'ance', 'ence', 'able', 'ible', 'ment', 'ant', 'ent', 'ism', 'ate', 'iti', 'ous', 'ive', 'ion', 'al', 'er', 'ic', 'ology')\n",
    "\n",
    "groups = [sgroup1, sgroup2, sgroup3, sgroup4, sgroup5, sgroup6]\n",
    "groups.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_word(word):\n",
    "    \n",
    "    word_out = word\n",
    "    \n",
    "    for group in groups:\n",
    "        \n",
    "        for suffix in group:\n",
    "            \n",
    "            if word.endswith(suffix) or (word+\"e\").endswith(suffix):\n",
    "                \n",
    "                offset = -len(suffix)\n",
    "                return word[:offset]\n",
    "            \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caresses: care\n",
      "flies: fl\n",
      "dies: d\n",
      "mules: mule\n",
      "denied: deni\n",
      "died: di\n",
      "agreed: agr\n",
      "owned: own\n",
      "humbled: humbl\n",
      "sized: siz\n",
      "meeting: meet\n",
      "stating: stat\n",
      "seizing: seiz\n",
      "itemization: itemizat\n",
      "sensational: sensation\n",
      "traditional: tradition\n",
      "reference: refer\n",
      "colonizer: coloniz\n",
      "plotted: plott\n",
      "guessed: guess\n",
      "chilling: chill\n"
     ]
    }
   ],
   "source": [
    "words = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died', \n",
    "         'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating', \n",
    "         'seizing', 'itemization', 'sensational', 'traditional', \n",
    "         'reference', 'colonizer', 'plotted', 'guessed', 'chilling']\n",
    "\n",
    "stems = [(word + \": \" + stem_word(word)) for word in words]\n",
    "print('\\n'.join(stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, using stemmers that take into account more rules and nuance are going to give better results. An algorithm developed by [Martin Porter](https://en.wikipedia.org/wiki/Martin_Porter) is more or less the *de facto* standard/baseline for English stemmers these days, and is of course built into NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caresses: caress\n",
      "flies: fli\n",
      "dies: die\n",
      "mules: mule\n",
      "denied: deni\n",
      "died: die\n",
      "agreed: agre\n",
      "owned: own\n",
      "humbled: humbl\n",
      "sized: size\n",
      "meeting: meet\n",
      "stating: state\n",
      "seizing: seiz\n",
      "itemization: item\n",
      "sensational: sensat\n",
      "traditional: tradit\n",
      "reference: refer\n",
      "colonizer: colon\n",
      "plotted: plot\n",
      "guessed: guess\n",
      "chilling: chill\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died', \n",
    "         'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating', \n",
    "         'seizing', 'itemization', 'sensational', 'traditional', \n",
    "         'reference', 'colonizer', 'plotted', 'guessed', 'chilling']\n",
    "\n",
    "stems = [(word + \": \" + stemmer.stem(word)) for word in words]\n",
    "print('\\n'.join(stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other approaches you can take to this, of course. One is to stop thinking about language-specific rules altogether and letting an algorithm find the patterns for you. The method presented below, an implementation of an old algorithm called Morfessor, is an **unsupervised machine learning** approach. That is, it an approach that takes a corpus and makes predictions about that data (in this case, where morpheme boundaries are) without being told anything explicit about the provided data.\n",
    "\n",
    "This approach loads a wordlist, goes through a training process, and then returns a model that can be used to segment new input based on what it learned from the initial wordlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ensure it's installed first: pip install morfessor\n",
    "import morfessor\n",
    "\n",
    "def train_model(input_file, output_file=None):\n",
    "\n",
    "    ## setup IO and model objects\n",
    "    morf_io = morfessor.MorfessorIO()\n",
    "    morf_model = morfessor.BaselineModel()\n",
    "\n",
    "    ## build a corpus from input file\n",
    "    train_data = morf_io.read_corpus_file(input_file)\n",
    "\n",
    "    ## load data into model\n",
    "    morf_model.load_data(train_data)\n",
    "\n",
    "    ## train the model in batch form (online training also available)\n",
    "    morf_model.train_batch()\n",
    "\n",
    "    ## optionally pickle model\n",
    "    if output_file is not None:\n",
    "        morf_io.write_binary_model_file(output_file, morf_model)\n",
    "\n",
    "    return morf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train a model on the Turkish dataset and save it to a file\n",
    "## be patient! it will be slow.\n",
    "# model_english = train_model(\"data/wordlist.eng\", \"output/trainedmodel.eng\")\n",
    "\n",
    "## otherwise, use the trained model if it's already saved\n",
    "morf_io = morfessor.MorfessorIO()\n",
    "model_english = morf_io.read_binary_model_file(\"data/trainedmodel.eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caresses : caress\n",
      "flies : flies\n",
      "dies : dies\n",
      "mules : mule\n",
      "denied : deni\n",
      "died : died\n",
      "agreed : agree\n",
      "owned : own\n",
      "humbled : humble\n",
      "sized : s\n",
      "meeting : meet\n",
      "stating : stating\n",
      "seizing : s\n",
      "itemization : item\n",
      "sensational : sensation\n",
      "traditional : traditional\n",
      "reference : reference\n",
      "colonizer : colon\n",
      "plotted : plot\n",
      "guessed : guess\n",
      "chilling : chilling\n"
     ]
    }
   ],
   "source": [
    "words = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died', \n",
    "         'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating', \n",
    "         'seizing', 'itemization', 'sensational', 'traditional', \n",
    "         'reference', 'colonizer', 'plotted', 'guessed', 'chilling']\n",
    "\n",
    "for word in words:\n",
    "    \n",
    "#     root = max(model_english.segment(word), key=len)\n",
    "    root = model_english.segment(word)[0]\n",
    "    print(word, \":\", root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Word Breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get simple probabilities from a corpus (Brown), stored in an easy-to-use collection (a frequency distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus_probabilities():\n",
    "\n",
    "    # get the total counts for each word in the corpus\n",
    "    counts = Counter(brown.words())\n",
    "\n",
    "    total = len(counts)\n",
    "\n",
    "    # overwrite the count of each word with its probability\n",
    "    for key in counts.keys():\n",
    "        counts[key] = counts[key] / total\n",
    "\n",
    "    return counts\n",
    "\n",
    "probabilities = get_corpus_probabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use a simple implementation of a [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) to efficiently search through the string and predict the most likely places for word breaks (spaces) to be inserted.\n",
    "\n",
    "Algorithms of this type ([dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming)) are used for a huge number of tasks in NLP, especially in those that involve **decoding** an unclear input (a sentence without word breaks, an audio speech signal, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_search(sent_str, fdist_obj):\n",
    "\n",
    "    sent_len = len(sent_str) + 1\n",
    "\n",
    "    # make a list to store the probability that a break happens after each letter in the sentence\n",
    "    probs = [0.0] * sent_len\n",
    "    \n",
    "    # the beginning of the sentence is a break, by definition\n",
    "    probs[0] = 1.0\n",
    "    \n",
    "    # have a list to store your potential start and end points for words, which are \n",
    "    # the complements of the break points\n",
    "    ranges = []\n",
    "\n",
    "    # outer loop is word ending index\n",
    "    for i in range(1, sent_len):\n",
    "\n",
    "        # inner loop is word beginning index\n",
    "        for j in range(0, i):\n",
    "            word = sent_str[j:i]\n",
    "            word_prob = fdist_obj[word]\n",
    "\n",
    "            # probs[j] is the current start letter's probability\n",
    "            test_prob = word_prob * probs[j]\n",
    "\n",
    "            if test_prob > probs[i]:\n",
    "                probs[i] = test_prob\n",
    "                ranges.append((j, i-1))\n",
    "\n",
    "    # now, pick only the word start/end pairs that don't overlap\n",
    "    ranges.reverse()\n",
    "    filtered = [ranges[0]]\n",
    "\n",
    "    for i in range(1, len(ranges)):\n",
    "\n",
    "        # prevents some slicing issues\n",
    "        if ranges[i] is None:\n",
    "            pass\n",
    "\n",
    "        elif ranges[i][1] == filtered[-1][0] - 1:\n",
    "            filtered.append(ranges[i])\n",
    "    \n",
    "    print(ranges)\n",
    "    print(filtered)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_result(sentence, ranges):\n",
    "    words = []\n",
    "\n",
    "    for r in ranges:\n",
    "        word = sentence[r[0]:r[1]+1]\n",
    "        words.append(word)\n",
    "\n",
    "    words.reverse()\n",
    "\n",
    "    print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 11), (8, 9), (7, 9), (8, 8), (4, 7), (6, 6), (5, 6), (4, 5), (1, 4), (0, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[(4, 11), (0, 3)]\n",
      "this sentence\n"
     ]
    }
   ],
   "source": [
    "# test_sentence = \"howlongofasentencecanweputherewithoutsomethingbreakingeverythingidon'tevenknowcanweextenditevenfurther\"\n",
    "test_sentence = \"thissentence\"\n",
    "\n",
    "word_ranges = viterbi_search(test_sentence, probabilities)\n",
    "print_result(test_sentence, word_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the probabilities already loaded, it's fast! But it breaks on words that it doesn't recognize, or that are **out of vocabulary** (OOV):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 8.919492659257541e-05, 3.182293971941967e-09, 1.1187362862800363, 0.0, 1.9957120186239655e-05, 1.9957120186239655e-05, 3.5601477400217023e-10, 3.5601477400217023e-10, 4.445659628619426e-14, 0.0020156691388102054, 1.0427672730523566e-06, 1.8601910074608996e-11, 1.8601910074608996e-11, 0.0, 0.0, 0.0, 7.191498434843839e-08, 8.98023244981124e-12, 4.494686521777399e-06, 8.018064687331465e-11, 2.8929177391891924e-06, 2.5803358538533923e-10, 9.206114682745749e-15, 3.2364120480541556e-06, 1.1546861401980683e-10, 1.1546861401980683e-10, 6.179528730745857e-15, 5.42702485893092e-09, 5.773430700990341e-10, 1.0969518331881647e-09, 0.0, 0.0, 0.0, 0.0, 1.0645268160164861e-11, 3.0918242082831767e-12, 1.6546501997697933e-16, 5.521567716631801e-13, 7.09193991111707e-16, 3.795390358626257e-20, 6.894941580252708e-17, 0.0, 1.9699833086436307e-17, 1.3002726228626993e-20, 2.3195544229314793e-25, 4.1378497296171386e-30, 7.0285006641227e-22, 3.7614396047537504e-26, 0.0, 6.269066007922917e-26, 5.591688823806944e-30, 9.839900929771779e-21]\n",
      "the current president of the united states is Teddy Roosevelt\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"thecurrentpresidentoftheunitedstatesisTeddyRoosevelt\"\n",
    "word_ranges = viterbi_search(test_sentence, probabilities)\n",
    "print_result(test_sentence, word_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>N-Grams</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myTokenizer(text):\n",
    "    '''Breaks up text into individual words and punction marks.'''\n",
    "    \n",
    "    puncs = ['.', ',', ';', '\"']\n",
    "    \n",
    "    # We will split on spaces, so we want punctuation separated.\n",
    "    for punc in puncs:\n",
    "        text = text.replace(punc, ' ' + punc)\n",
    "        \n",
    "    split_txt = text.split()\n",
    "    \n",
    "    return split_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_prob = \"This sentence, even with punctuation, should work just fine.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sentence', ',', 'even', 'with', 'punctuation', ',', 'should', 'work', 'just', 'fine', '.']\n"
     ]
    }
   ],
   "source": [
    "print(myTokenizer(no_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = \"This'll have problems because of the contractions; it isn't gonna work as well.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"This'll\", 'have', 'problems', 'because', 'of', 'the', 'contractions', ';', 'it', \"isn't\", 'gonna', 'work', 'as', 'well', '.']\n"
     ]
    }
   ],
   "source": [
    "print(myTokenizer(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens1 = word_tokenize(no_prob)\n",
    "tokens2 = word_tokenize(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sentence', ',', 'even', 'with', 'punctuation', ',', 'should', 'work', 'just', 'fine', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', \"'ll\", 'have', 'problems', 'because', 'of', 'the', 'contractions', ';', 'it', 'is', \"n't\", 'gon', 'na', 'work', 'as', 'well', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x115fffbf8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = ngrams(tokens2, 2)\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', \"'ll\")\n",
      "(\"'ll\", 'have')\n",
      "('have', 'problems')\n",
      "('problems', 'because')\n",
      "('because', 'of')\n",
      "('of', 'the')\n",
      "('the', 'contractions')\n",
      "('contractions', ';')\n",
      "(';', 'it')\n",
      "('it', 'is')\n",
      "('is', \"n't\")\n",
      "(\"n't\", 'gon')\n",
      "('gon', 'na')\n",
      "('na', 'work')\n",
      "('work', 'as')\n",
      "('as', 'well')\n",
      "('well', '.')\n"
     ]
    }
   ],
   "source": [
    "for b in bigrams:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', \"'ll\", 'have')\n",
      "(\"'ll\", 'have', 'problems')\n",
      "('have', 'problems', 'because')\n",
      "('problems', 'because', 'of')\n",
      "('because', 'of', 'the')\n",
      "('of', 'the', 'contractions')\n",
      "('the', 'contractions', ';')\n",
      "('contractions', ';', 'it')\n",
      "(';', 'it', 'is')\n",
      "('it', 'is', \"n't\")\n",
      "('is', \"n't\", 'gon')\n",
      "(\"n't\", 'gon', 'na')\n",
      "('gon', 'na', 'work')\n",
      "('na', 'work', 'as')\n",
      "('work', 'as', 'well')\n",
      "('as', 'well', '.')\n"
     ]
    }
   ],
   "source": [
    "trigrams = ngrams(tokens2, 3)\n",
    "for t in trigrams:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', \"'ll\", 'have', 'problems', 'because', 'of', 'the', 'contractions')\n",
      "(\"'ll\", 'have', 'problems', 'because', 'of', 'the', 'contractions', ';')\n",
      "('have', 'problems', 'because', 'of', 'the', 'contractions', ';', 'it')\n",
      "('problems', 'because', 'of', 'the', 'contractions', ';', 'it', 'is')\n",
      "('because', 'of', 'the', 'contractions', ';', 'it', 'is', \"n't\")\n",
      "('of', 'the', 'contractions', ';', 'it', 'is', \"n't\", 'gon')\n",
      "('the', 'contractions', ';', 'it', 'is', \"n't\", 'gon', 'na')\n",
      "('contractions', ';', 'it', 'is', \"n't\", 'gon', 'na', 'work')\n",
      "(';', 'it', 'is', \"n't\", 'gon', 'na', 'work', 'as')\n",
      "('it', 'is', \"n't\", 'gon', 'na', 'work', 'as', 'well')\n",
      "('is', \"n't\", 'gon', 'na', 'work', 'as', 'well', '.')\n"
     ]
    }
   ],
   "source": [
    "eightgrams = ngrams(tokens2, 8)\n",
    "for e in eightgrams:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's Play With a Corpus</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural, brown\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addS(text):\n",
    "    '''Add open and closed sentence tags to every sentence in a corpus.'''\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        text[i].insert(0, '<s>')\n",
    "        text[i].append('</s>')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nextWord(seed, BGs):\n",
    "    '''Find all words that follow a chosen word. Randomly choose one.'''\n",
    "    \n",
    "    choices = [words[1] for words in BGs if words[0] == seed]\n",
    "    random.shuffle(choices)\n",
    "    \n",
    "    return choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tgNextWord(seed1, seed2, TGs):\n",
    "    '''Find all words that follow two chosen words. Randomly choose one.'''\n",
    "    \n",
    "    choices = [words[2] for words in TGs if words[0] == seed1 and words[1] == seed2]\n",
    "    random.shuffle(choices)\n",
    "    \n",
    "    return choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.ConcatenatedCorpusView"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inaugural.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':']\n"
     ]
    }
   ],
   "source": [
    "print(inaugural.sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inaug = addS(list(inaugural.sents()))\n",
    "rom = addS(list(brown.sents(categories='romance')))\n",
    "BGList = []\n",
    "for sent in inaug:\n",
    "    bgs = list(ngrams(sent, 2))\n",
    "    # We now have a list of lists. We just want one list.\n",
    "    for b in bgs:\n",
    "        BGList.append(b)\n",
    "for sent in rom:\n",
    "    bgs = list(ngrams(sent, 2))\n",
    "    for b in bgs:\n",
    "        BGList.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'Fellow'), ('Fellow', '-'), ('-', 'Citizens')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BGList[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is said , our lives . </s> "
     ]
    }
   ],
   "source": [
    "# Now we can generate sentences.\n",
    "# 1) Start with the open sentence tag as a seed.\n",
    "# 2) Randomly pick a word from the corpus that can follow the seed.\n",
    "# 3) That word becomes the next seed.\n",
    "# 4) Repeat until close sentence tag.\n",
    "seed = '<s>'\n",
    "while seed != '</s>':\n",
    "    word = nextWord(seed, BGList)\n",
    "    print(word, end=' ')\n",
    "    seed = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as with the bigrams above, but with trigrams this time.\n",
    "# Let's also expand our corpus a bit.\n",
    "sci = addS(list(brown.sents(categories='science_fiction')))\n",
    "adv = addS(list(brown.sents(categories='adventure')))\n",
    "TGList = []\n",
    "for sent in inaug:\n",
    "    tgs = list(ngrams(sent, 3))\n",
    "    # We now have a list of lists. We just want one list.\n",
    "    for t in tgs:\n",
    "        TGList.append(t)\n",
    "for sent in rom:\n",
    "    tgs = list(ngrams(sent, 3))\n",
    "    for t in tgs:\n",
    "        TGList.append(t)\n",
    "for sent in sci:\n",
    "    tgs = list(ngrams(sent, 3))\n",
    "    for t in tgs:\n",
    "        TGList.append(t)\n",
    "for sent in adv:\n",
    "    tgs = list(ngrams(sent, 3))\n",
    "    for t in tgs:\n",
    "        TGList.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When one surveys the world , we can not be precisely such as these can aid the fulfillment of my humble abilities to their exertions in the outset for the cup of good government , whether you are ! ! </s> "
     ]
    }
   ],
   "source": [
    "# Trigrams should produce slightly more grammatical results.\n",
    "# For this, we need to generate the first word with bigrams.\n",
    "# Then we can use \"<s> + FirstWord\" as our seed.\n",
    "firstWord = nextWord('<s>', BGList)\n",
    "seed1 = '<s>'\n",
    "seed2 = firstWord\n",
    "print(firstWord, end=' ')\n",
    "while seed2 != '</s>':\n",
    "    word = tgNextWord(seed1, seed2, TGList)\n",
    "    print(word, end=' ')\n",
    "    seed1, seed2 = seed2, word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import log\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CorpusDataset(object):\n",
    "\n",
    "    def __init__(self, corpus_obj, filter_oovs=False):\n",
    "\n",
    "        # pulls appropriate sentences based on init values\n",
    "        self.sents = corpus_obj.tagged_sents(tagset=\"universal\")\n",
    "\n",
    "        if filter_oovs is True:\n",
    "            self.sents = self.oov_smoother()\n",
    "\n",
    "        # init ngrams\n",
    "        self.unigram_tagfd, self.unigram_cfd, self.bigram_cfd = self.set_ngrams()\n",
    "\n",
    "        # set probabilities\n",
    "        self.word_condprob = self.set_probabilities(self.unigram_cfd, \"word\")\n",
    "        self.tag_condprob = self.set_probabilities(self.bigram_cfd, \"tag\")\n",
    "\n",
    "    def oov_smoother(self):\n",
    "        # filters through self.sents and replaces first instances of any token\n",
    "        # with u'<unk>', in order to deal with OOV problem\n",
    "\n",
    "        # set is used in lieu of a list for speed\n",
    "        tracker = set()\n",
    "        out_sents = []\n",
    "\n",
    "        # filters through each layer, using tracker to test for first instance\n",
    "        for sentence in self.sents:\n",
    "            sent_new = []\n",
    "\n",
    "            for word_tag in sentence:\n",
    "\n",
    "                if word_tag[0] not in tracker:\n",
    "                    tracker.add(word_tag[0])\n",
    "                    sent_new.append((u'<UNK>', word_tag[1]))\n",
    "\n",
    "                else:\n",
    "                    sent_new.append(word_tag)\n",
    "\n",
    "            out_sents.append(sent_new)\n",
    "\n",
    "        return out_sents\n",
    "\n",
    "    def set_ngrams(self):\n",
    "\n",
    "        unigram_tag_fd = defaultdict(int)\n",
    "        unigram_tag_cfd = {}\n",
    "        bigram_tag_cfd = {}\n",
    "\n",
    "        for sentence in self.sents:\n",
    "\n",
    "            # for sentence in sentence_group:\n",
    "            # print(sentence)\n",
    "\n",
    "            # add sentence start and end tags (and a corresponding\n",
    "            # \"part of speech\" for both of them)\n",
    "            sentence.insert(0, (u'<s>', u'<s>'))\n",
    "            sentence.append((u'</s>', u'</s>'))\n",
    "\n",
    "            unigram_tag_fd[sentence[0][1]] += 1\n",
    "\n",
    "            # start iterating from index 1 so that you always\n",
    "            # have a previous tag to look at\n",
    "            for i in range(1, len(sentence)):\n",
    "\n",
    "                # drastically improves readability below\n",
    "                word = sentence[i][0]\n",
    "                tag = sentence[i][1]\n",
    "                prev_tag = sentence[i-1][1]\n",
    "\n",
    "                # sets tag frequencies\n",
    "                unigram_tag_fd[tag] += 1\n",
    "\n",
    "                # sets conditional counts for word given tag\n",
    "                if word not in unigram_tag_cfd:\n",
    "                    unigram_tag_cfd[word] = defaultdict(int)\n",
    "\n",
    "                unigram_tag_cfd[word][tag] += 1\n",
    "\n",
    "                # sets conditional counts for tag given previous tags\n",
    "                if tag not in bigram_tag_cfd:\n",
    "                    bigram_tag_cfd[tag] = defaultdict(int)\n",
    "\n",
    "                bigram_tag_cfd[tag][prev_tag] += 1\n",
    "\n",
    "        return unigram_tag_fd, unigram_tag_cfd, bigram_tag_cfd\n",
    "\n",
    "    def set_probabilities(self, cfd, cfd_type):\n",
    "\n",
    "        if cfd_type == \"word\":\n",
    "            for word in cfd:\n",
    "                for tag in cfd[word]:\n",
    "                    cfd[word][tag] = log(cfd[word][tag] /\n",
    "                                         self.unigram_tagfd[tag])\n",
    "\n",
    "        elif cfd_type == \"tag\":\n",
    "            for tag in cfd:\n",
    "                for prev_tag in cfd[tag]:\n",
    "                    cfd[tag][prev_tag] = log(cfd[tag][prev_tag] /\n",
    "                                             self.unigram_tagfd[tag])\n",
    "\n",
    "        return cfd\n",
    "\n",
    "    def get_word_cfd(self):\n",
    "        \"\"\"Get a conditional frequency distribution for words given a tag.\"\"\"\n",
    "        return self.word_condprob\n",
    "\n",
    "    def get_tag_cfd(self):\n",
    "        \"\"\"Get a conditional frequency distribution for a tag given a previous tag.\"\"\"\n",
    "        return self.tag_condprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_search_matrices(sentence, tagset, tag_cfd, word_cfd):\n",
    "    \"\"\"Builds the state and trace matrices for the viterbi algorithm.\n",
    "\n",
    "    A state matrix (a numpy array, with axis 0 as word and axis 1 as POS) and\n",
    "    a backtrace matrix (a list) are constructed for the input sequence and the\n",
    "    universal tagset. After initialization, probabilities for the first input\n",
    "    word and each POS are added to the state matrix, and the first POS is added\n",
    "    to the traces matrix.\n",
    "\n",
    "    Returns state matrix and traces list for viterbi_search()\n",
    "    \"\"\"\n",
    "\n",
    "    # set a fallback POS for cases where a tag-tag bigram doesn't exist\n",
    "    fallback_pos = 'NOUN'\n",
    "\n",
    "    # initialize matrices based on length of input sentence and number of tags\n",
    "    states = np.zeros((len(sentence), len(tagset)))\n",
    "    traces = []\n",
    "\n",
    "    # set first column transition probabilities\n",
    "    for i in range(len(tagset)):\n",
    "\n",
    "        # added for more readable code below\n",
    "        tag = tagset[i]\n",
    "        first_word = sentence[0]\n",
    "\n",
    "        # OOV variable for correct fallback assignment\n",
    "        first_word_is_oov = False\n",
    "\n",
    "        word_prob = 0.0\n",
    "        # test for given word\n",
    "        if sentence[0] in word_cfd:\n",
    "            word_prob = word_cfd[first_word][tag]\n",
    "\n",
    "        # test lowercased if previous fails\n",
    "        elif sentence[0].lower() in word_cfd:\n",
    "            word_prob = word_cfd[first_word.lower()][tag]\n",
    "\n",
    "        # set the OOV variable to true so that fallback is assigned\n",
    "        else:\n",
    "            first_word_is_oov = True\n",
    "\n",
    "        # set the tag probability for beginning a sentence\n",
    "        try:\n",
    "            tag_prob = tag_cfd[tag]['<s>']\n",
    "        # if the lookup fails, use the fallback POS tag\n",
    "        except KeyError:\n",
    "            try:\n",
    "                tag_prob = tag_cfd[tag][fallback_pos]\n",
    "            # if THIS lookup fails, use the default val (see next comment)\n",
    "            except KeyError:\n",
    "                tag_prob = -18.0 * (i + 1)\n",
    "\n",
    "        # this series deals with word/tag probabilities that end up as 0\n",
    "        # because adding log probabilities doesn't have the multiply-by-zero\n",
    "        # failsafe that multiplying probabilities does, is used as a floor\n",
    "        # value; it approximates the lowest probability for a value in the\n",
    "        # data\n",
    "        if word_prob == 0.0:\n",
    "            states[0][i] = -18.0\n",
    "        elif tag_prob == 0.0:\n",
    "            states[0][i] = -18.0\n",
    "        else:\n",
    "            states[0][i] = word_prob + tag_prob\n",
    "\n",
    "    # sets backtrace as fallback POS if the word is OOV\n",
    "    # this was necessary to prevent certain errors, since the most common\n",
    "    # sentence initial unknown POSs were things like numbers, which throws off\n",
    "    # the rest of the algorithm\n",
    "    # if first_word_is_oov is True:\n",
    "    #     traces.append(fallback_pos)\n",
    "    # else:\n",
    "    # np.argmax gives the index for the highest value in an array\n",
    "    max_prob = np.argmax(states[0])\n",
    "    best_pos = tagset[max_prob]\n",
    "    traces.append(best_pos)\n",
    "\n",
    "    return states, traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_part_of_speech(sentence, tagset, tag_cfd, word_cfd):\n",
    "    \"\"\"Returns a POS tag sequence for an input sentence.\n",
    "\n",
    "    This is similar in implementation to initMatrices() above. For each word\n",
    "    and each POS, three log probabilities are added (probability of a word\n",
    "    given a tag, probability of a tag given a previous tag, and the previous\n",
    "    word's probability with its best tag). The POS with the highest probability\n",
    "    for a word is then added to the traces list, which is returned. The <UNK>\n",
    "    tag is used as a standin for OOV words, and NOUN/NN is used as a fallback\n",
    "    tag for problems with previous tag probability.\n",
    "\n",
    "    Returns a list of POS tags corresponding to the input string.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the matrices for tracking input\n",
    "    states, traces = get_search_matrices(sentence, tagset, tag_cfd, word_cfd)\n",
    "\n",
    "    # use NOUN as the default tag\n",
    "    fallback_pos = 'NOUN'\n",
    "\n",
    "    for i in range(1, len(sentence)):\n",
    "        for j in range(len(tagset)):\n",
    "\n",
    "            # the current word and POS being checked\n",
    "            # added for more readable code below\n",
    "            word = sentence[i]\n",
    "            tag = tagset[j]\n",
    "\n",
    "            # test to see if the current word is in the CFD\n",
    "            # if it is, get it's probability given the current tag\n",
    "            if word in word_cfd:\n",
    "                word_prob = word_cfd[word][tag]\n",
    "\n",
    "            # test the same word but lowercased if the previous fails\n",
    "            elif word.lower() in word_cfd:\n",
    "                word_prob = word_cfd[word.lower()][tag]\n",
    "\n",
    "            # take the probability for an unknown given this tag\n",
    "            else:\n",
    "                word_prob = word_cfd['<UNK>'][tag]\n",
    "\n",
    "            # tag probability given previous tag\n",
    "            # because traces is being incrementally filled, get its last entry\n",
    "            try:\n",
    "                tag_prob = tag_cfd[tag][traces[-1]]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    tag_prob = tag_cfd[tag][fallback_pos]\n",
    "                except KeyError:\n",
    "                    tag_prob = -18.0 * (i + 1)\n",
    "\n",
    "            # probability of previous word\n",
    "            prev_word_prob = np.argmax(states[i - 1])\n",
    "\n",
    "            if word_prob == 0.0:\n",
    "                states[i][j] = -18.0 * (i + 1)\n",
    "            elif tag_prob == 0.0:\n",
    "                states[i][j] = -18.0 * (i + 1)\n",
    "            else:\n",
    "                prob = word_prob + tag_prob + prev_word_prob\n",
    "                states[i][j] = prob\n",
    "\n",
    "        # gets the index of the part of speech with the highest probability\n",
    "        max_prob = np.argmax(states[i])\n",
    "\n",
    "        # pulls the name of that part of speech out using that index\n",
    "        best_pos = tagset[max_prob]\n",
    "\n",
    "        # add the best POS to the eventual output\n",
    "        traces.append(best_pos)\n",
    "\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "universal_tagset = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']\n",
    "\n",
    "trained_corpus = CorpusDataset(brown, filter_oovs=True)\n",
    "word_cfd = trained_corpus.get_word_cfd()\n",
    "tag_cdf = trained_corpus.get_tag_cfd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET VERB DET ADJ NOUN .\n"
     ]
    }
   ],
   "source": [
    "# sentence = word_tokenize(\"Time flies like an arrow, fruit flies like a banana.\")\n",
    "sentence = word_tokenize(\"What is your favorite color?\")\n",
    "\n",
    "\n",
    "pos_list = get_part_of_speech(sentence, universal_tagset, tag_cdf, word_cfd)\n",
    "\n",
    "print(\" \".join(pos_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Similar Words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "\n",
    "corpus = Text(inaugural.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of and to by for that with from on is as at upon under all but when\n",
      "among within through\n"
     ]
    }
   ],
   "source": [
    "# Find words in the same context as a given word.\n",
    "# Context = Previous-Word, Our-Word, Following-Word\n",
    "corpus.similar('in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it freedom peace government all god congress life them justice which\n",
      "others us war people nations law power this interest\n"
     ]
    }
   ],
   "source": [
    "corpus.similar('America')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "government peace them nation life congress people war union america it\n",
      "freedom all which men us god nations duty power\n"
     ]
    }
   ],
   "source": [
    "corpus.similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discussion opinions who discord sincerity canals death that\n",
      "maintaining prosperity women received dependent relying government\n",
      "nothing he riders irresponsibility this\n"
     ]
    }
   ],
   "source": [
    "corpus.similar('woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>WordNet</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'domestic_dog', 'Canis_familiaris']\n",
      "['chase', 'chase_after', 'trail', 'tail', 'tag', 'give_chase', 'dog', 'go_after', 'track']\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').lemma_names())\n",
    "print(wn.synset('dog.v.01').lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "go after with the intent to catch\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').definition())\n",
    "print(wn.synset('dog.v.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the dog barked all night']\n",
      "['The policeman chased the mugger down the alley', 'the dog chased the rabbit']\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('dog.n.01').examples())\n",
    "print(wn.synset('dog.v.01').examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('basenji.n.01'),\n",
       " Synset('corgi.n.01'),\n",
       " Synset('cur.n.01'),\n",
       " Synset('dalmatian.n.02'),\n",
       " Synset('great_pyrenees.n.01'),\n",
       " Synset('griffon.n.02'),\n",
       " Synset('hunting_dog.n.01'),\n",
       " Synset('lapdog.n.01'),\n",
       " Synset('leonberg.n.01'),\n",
       " Synset('mexican_hairless.n.01'),\n",
       " Synset('newfoundland.n.01'),\n",
       " Synset('pooch.n.01'),\n",
       " Synset('poodle.n.01'),\n",
       " Synset('pug.n.01'),\n",
       " Synset('puppy.n.01'),\n",
       " Synset('spitz.n.01'),\n",
       " Synset('toy_dog.n.01'),\n",
       " Synset('working_dog.n.01')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('dog.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('feline.n.01')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('cat.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('domestic_cat.n.01'), Synset('wildcat.n.03')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('cat.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('carnivore.n.01')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('feline.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('domestic_cat.n.01'),\n",
       " Synset('feeder.n.01'),\n",
       " Synset('head.n.02'),\n",
       " Synset('stocker.n.01'),\n",
       " Synset('stray.n.01')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('domestic_animal.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.3333333333333333\n",
      "0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "d_cat = wn.synset('domestic_cat.n.01')\n",
    "truck = wn.synset('truck.n.01')\n",
    "print(dog.path_similarity(cat))\n",
    "print(dog.path_similarity(d_cat))\n",
    "print(dog.path_similarity(truck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
